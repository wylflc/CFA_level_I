# 1 Quantitative Methods 量化方法
## 1.10 Simple Linear Regression 简单线性回归
### 1.10.3 Hypothesis Tests in the Simple Linear Regression Model 简单线性回归模型中的假设检验

#### 方差分析（Analysis of Variance）

简单线性回归模型有时能够很好地描述两个变量之间的关系，但有时却不能。为了有效地使用回归分析，我们必须能够区分这两种情况。回归分析的目标是解释因变量的变异。那么，考虑到我们选择的自变量，模型能够多好地达成这一目标？

#### 分解总平方和

首先，我们从**总平方和**（Sum of Squares Total, SST）开始，然后将其分解为两个部分：**误差平方和**（Sum of Squares Error, SSE）和**回归平方和**（Sum of Squares Regression, SSR）。

其中：
- **总平方和**（SST）：表示因变量Y的总变异。 $$ SST = \sum_{i=1}^{n} ( Y_i - \bar{Y} )^2 $$
- **误差平方和**（SSE）：表示未被回归模型解释的变异。 $$
   SSE = \sum_{i=1}^{n} (Y_i - \hat{Y}_i)^2
   $$
- **回归平方和**（SSR）：表示回归模型解释的变异。 $$
   SSR = \sum_{i=1}^{n} (\hat{Y}_i - \bar{Y})^2
   $$
**方程**：
$$ SST = SSR + SSE $$

这意味着，因变量Y的总变异等于由回归模型解释的变异（SSR）加上未被解释的变异（SSE）。

**总结**：
- 方差分析的目的是理解回归模型解释了多少因变量的变异。
- SST、SSR和SSE的关系可以帮助我们评估回归模型的拟合效果。

#### 拟合优度的度量（Measures of Goodness of Fit）

为了评估回归模型对数据的拟合程度（即模型对数据的适应性），我们可以使用几种度量标准，包括**决定系数**（$R^2$）、**F统计量**以及**回归标准误差**。

**1. 决定系数（$R^2$）**：
决定系数（$R^2$）表示因变量变异中由自变量解释的百分比：
$$ R^2 = \frac{ \sum_{i=1}^{n} (\hat{Y_i} - \bar{Y})^2 }{ \sum_{i=1}^{n} (Y_i - \bar{Y})^2 } $$
由此，决定系数的范围从0%到100%。例如如果决定系数为$R^2=0.8001$，则意味着**80.01%** 的因变量可以通过自变量来解释。对于简单线性回归，**样本相关系数的平方**等于决定系数：
$$ r^2 = R^2 $$

**2. F统计量**：
决定系数虽然能够描述因变量由自变量解释的变异比例，但它本身并不是统计检验。为了判断回归模型是否在统计学上有意义，我们需要构建一个F分布的检验统计量。

一般来说，**F统计量用于比较两个方差**。在回归分析中，我们使用F统计量来检验回归中的斜率是否等于零，检验的原假设是所有斜率均为零：

- **原假设**（$H_0$）：$b_1 = b_2 = \dots = b_k = 0$
- **备择假设**（$H_a$）：至少有一个 $b_k$不等于零。

对于简单线性回归，这些假设简化为：

- **原假设**（$H_0$）：$b_1 = 0$
- **备择假设**（$H_a$）：$b_1 \neq 0$

F统计量的构建通过使用回归平方和和误差平方和，调整自由度进行计算，即它是两个方差的比值：

1. **均方回归（MSR）**：回归平方和除以自变量个数 $k$（对于简单线性回归，$k = 1$）：
$$ MSR = \frac{ \sum_{i=1}^{n} (\hat{Y_i} - \bar{Y})^2 }{ k } $$
   对于简单线性回归：
$$ MSR = \sum_{i=1}^{n} (\hat{Y_i} - \bar{Y})^2 $$

2. **均方误差（MSE）**：误差平方和除以自由度 $n - k - 1$（对于简单线性回归，$n - k - 1 = n - 2$）：
$$ MSE = \frac{ \sum_{i=1}^{n} (Y_i - \hat{Y_i})^2 }{ n - k - 1 } $$
   对于简单线性回归：
$$ MSE = \frac{ \sum_{i=1}^{n} (Y_i - \hat{Y_i})^2 }{ n - 2 } $$
3. **F统计量**：
$$ F = \frac{MSR}{MSE} = \frac{ \sum_{i=1}^{n} (\hat{Y_i} - \bar{Y})^2 / k }{ \sum_{i=1}^{n} (Y_i - \hat{Y_i})^2 / (n - 2) } $$

   在简单线性回归中，F统计量遵循一个F分布，具有1和$n - 2$的自由度。F统计量是单侧的，其拒绝域位于右侧，因为我们关心的是Y的变异（分子）是否大于未解释的Y变异（分母）。
   
   **F检验的结论**
   - **F统计量较大**，表明回归模型能够较好地解释因变量的变异，自变量的整体显著性较强，通常会拒绝零假设（H0H_0H0​：所有回归系数为零，表示自变量对因变量没有影响）。
   - **F统计量较小**，则说明回归模型的整体显著性差，无法拒绝零假设，可能意味着自变量对因变量的解释力弱。

#### 回归系数的假设检验（Hypothesis Testing of Individual Regression Coefficients）

**1. 斜率系数的假设检验（Hypothesis Tests of the Slope Coefficient）**：
我们可以使用F统计量来检验斜率系数的显著性（即是否显著不等于零），但也可能希望对斜率系数进行其他假设检验——例如，检验总体斜率是否与某个特定值不同，或者检验斜率是否为正。我们可以使用t分布的检验统计量来检验关于回归系数的假设。

假设我们想要检查股票的估值，使用市场模型；我们假设股票的系统风险平均水平（即与市场相似的风险），该风险由市场回报变量的系数表示。或者，我们可能想检验经济学家对通货膨胀率的预测是否是无偏的（即在平均水平上，不高估或低估实际通货膨胀率）。在这些情况下，证据是否支持这些假设？这些问题可以通过对回归斜率的假设检验来解决。为了检验斜率的假设，我们计算检验统计量，方法是将假设的总体斜率（$B_1$）与估计的斜率系数（$\hat{b_1}$）之间的差值除以斜率系数的标准误差（$s_{\hat{b_1}}$）：

$$ t = \frac{\hat{b_1} - B_1}{s_{\hat{b_1}}} $$

该检验统计量遵循t分布，自由度为$n - k - 1$或$n - 2$，因为回归中估计了两个参数（截距和斜率）。

**2. 斜率系数的标准误差（Standard Error of the Slope Coefficient）**：
对于简单线性回归，斜率系数的标准误差是模型估计的标准误差（$s_e$）与自变量变异平方根的比值：

$$ s_{\hat{b_1}} = \frac{s_e}{\sqrt{\sum_{i=1}^{n} (X_i - \bar{X})^2}} $$

**3. 检验过程**：
我们通过比较计算的$t$统计量与临界$t$值来检验假设。值得注意的是，自变量的变异性越大，斜率的标准误差就越小，因此计算出的$t$统计量就越大。如果计算出的t统计量超出了临界t值的范围，我们拒绝原假设；如果计算的t统计量在临界值范围内，我们则无法拒绝原假设。与均值的检验类似，备择假设可以是双侧的或单侧的。

考虑我们之前的简单线性回归示例，其中ROA是因变量，CAPEX是自变量。假设我们想要检验CAPEX的斜率系数是否不同于零，以确认ROA与CAPEX之间的显著关系。我们可以使用六步过程来检验斜率的假设。通过此检验，我们得出结论：斜率不同于零；也就是说，CAPEX是ROA的一个显著解释变量。

**4. 斜率和相关性的关系**：
简单线性回归的一个特点是，用来检验斜率系数是否等于零的t统计量，与检验相关性是否为零的t统计量（即$H_0: \rho = 0$与$H_a: \rho \neq 0$）是相同的值。与斜率检验类似，相关性的检验也可以是双侧或单侧的。例如，$H_0: \rho \leq 0$与$H_a: \rho > 0$。用于检验相关性是否为零的检验统计量如下：

$$ t = \frac{r \sqrt{n - 2}}{\sqrt{1 - r^2}} $$

在我们ROA与CAPEX回归的示例中，相关系数$r = 0.8945$。要检验该相关性是否不同于零，我们进行假设检验，得出结论：ROA与CAPEX之间存在显著相关性。

**5. 斜率和相关性的关系**：
简单线性回归的另一个有趣特点是，用来检验模型拟合度的检验统计量（即F分布的检验统计量）与检验斜率系数是否等于零的计算t统计量是相关的：$t^2 = F$；因此，$4.001312 = 16.0104$。

**6. 检验一对一关系**：
假设我们想检验ROA与CAPEX之间是否存在线性关系，斜率系数为1.0。假设变为：$H_0: b_1 = 1$，$H_a: b_1 \neq 1$。计算出的t统计量如下：

$$ t = \frac{1.25 - 1.0}{0.312398} = 0.80026 $$

这个计算出来的检验统计量落在临界值范围内，$±2.776$，因此我们无法拒绝原假设：没有足够的证据表明斜率不同于1.0。

